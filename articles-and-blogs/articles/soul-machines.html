<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-176158185-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-176158185-1');
</script>

		<title>Soul Machines | StarBase </title>
		<!-- link to website tab icon -->
		<link rel="shortcut icon" type="image/x-icon" href="/../docs/assets/images/alt-logo-star-tilted.png">
		<!-- link to Main Stylesheets -->
		<link rel="stylesheet" type="text/css" href="/../docs/assets/css/styles.css">
		<!-- link to Google Fonts -->
		<link href='https://fonts.googleapis.com/css?family=Zilla Slab' rel='stylesheet'>
		<link href="https://fonts.googleapis.com/css2?family=Marcellus&display=swap" rel="stylesheet"> 
		<link href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro&display=swap" rel="stylesheet"> 
		<link href="https://fonts.googleapis.com/css2?family=Fraunces:wght@200..400&display=swap" rel="stylesheet">
		<!-- link to Font awesome -->
		<script src="https://kit.fontawesome.com/378306bc97.js" crossorigin="anonymous"></script>
		<!-- link to Material Icons -->
		<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
		<!-- link to Iconify CDN -->
		<script src="https://code.iconify.design/1/1.0.7/iconify.min.js"></script>
		<!-- link to UUID -->	
		<script src="https://cdnjs.cloudflare.com/ajax/libs/uuid/8.1.0/uuidv5.min.js"></script>
	</head>
	<body>
		<a id="top"></a>
			<!-- Header Holder --> 

<div class="header-holder has-default-focus"
  
  >
    <div class="nav-bar">
        <!---->
        <a class="nav-bar-button has-padding-left-medium has-padding-left-none-tablet" href="https://sachin1882.wixsite.com/startechenterprises/guides" title="Go to parent site: StarTech Enterprises">
            <div class="nav-bar-logo has-background-image">
                <img src="https://fontmeme.com/permalink/200623/587d8aec23ed76c10e0b1ec43bc85f23.png" class="alt-logo has-background-image" width="180" height ="70">
                <div class="overlay">
                    <img src="/../docs/assets/images/starbase_logo.png" class="nav-bar-logo">
                </div>
            </div>
        </a>
        <nav class="nav-bar-nav">
            <ul class="nav-bar-nav-list">
                <li class="nav-bar-item is-category has-spacing">
                    <a class = "nav-bar-button title has-hover-underline is-6 
                    
                    
                    " href="/docs/">
                        <span>
                            <!---->
                            Docs
                            <!---->
                        </span>
                    </a>
                </li>
                <li class="nav-bar-item">
                    <a class = "nav-bar-button has-hover-underline 
                    
                    
                    " href="/docs/">
                        <span>
                            <!---->
                            Guides
                            <!---->
                        </span>
                    </a>
                </li>
                <li class="nav-bar-item">
                    <a class = "nav-bar-button has-hover-underline 
                    
                    underline
                    
                    " href="/docs/articles-and-blogs/article-search.html">
                        <span>
                            <!---->
                            Articles & Blogs
                            <!---->
                        </span>
                    </a>
                </li>
                <li class="nav-bar-item">
                    <a class = "nav-bar-button has-hover-underline" href="/docs/videos/">
                        <span>
                            <!---->
                            Videos
                            <!---->
                        </span>
                    </a>
                </li>
                <li class="nav-bar-item">
                    <a class = "nav-bar-button has-hover-underline 
                    
                    
                    " href="/docs/about/about.html">
                        <span>
                            <!---->
                            About
                            <!---->
                        </span>
                    </a>
                </li>
            </ul>
        </nav>
        <span class = "nav-bar-spacer i-hidden-mobile"></span>
        <!---->
    </div>
</div>


<div class="header-holder transparent has-default-focus" style="position: sticky; top: 0; left: 0; right: 0;">
    <div class="content-header uhf-container has-padding has-default-focus is-flexbox" style="padding-top: 0px; padding-bottom: 1px;">
        
<!-- Get correct Breadcrumb Links from Front Matter of loaded page-->

<!-- Liquid Code -->
    <!-- Page Root -->
    

    <!-- Page Type -->
    

    <!-- Page Category -->
    

    <!-- Page Series -->
    
<!-- / End of Liquid Code -->


<!-- Breadcrumb Links --> 
<nav class = "has-padding-none has-padding-left-medium-tablet has-padding-right-medium-tablet has-padding-left-none-uhf-tablet has-padding-left-none-uhf-tablet has-padding-none-desktop has-flex-grow">
    <ul id="page-breadcrumbs" class="breadcrumbs">
        <li><a href="/docs/"><span class ="">Docs</span></a></li>
        <li><a href="/docs/articles-and-blogs/article-search.html"><span class ="">Articles & Blogs</span></a></li>
        <li><a href=""><span class="">Main Category</span></a></li>
        <li><a href=><span class="">Main Series</span></a></li>

        <!-- This part needs to be generated by JavaScript-->

    </ul>
</nav>
<!-- end of / Breadcrumb links -->




        <!-- Page Functions --> 

<div class="has-padding-none-tablet has-padding-medium is-size-7 is-flex-touch has-flex-justify-content-space-between-touch has-flex-grow">
    <ul class="is-hidden-mobile action-list has-flex-justify-content-end-tablet is-flex is-flex-row has-flex-wrap has-flex-grow is-unstyled">
        <li>
            <button class="print button is-text has-inner-focus is-small is-icon-only-touch colorfill" type="button" title="Print this page" onclick="window.print()">
                <span class="icon">
                    <span class="iconify extra-small" data-icon="el:print" data-inline="false"></span>                </span>
                <span class="is-visually-hidden-touch is-hidden-portrait">Print</span>
            </button>
        </li>
        <li>
            <button class="button is-text has-inner-focus is-small is-icon-only-touch colorfill" title="Send feedback about this page" onclick="location.href='#feedback'">
                <span class="icon">
                    <i class="fas fa-comments"></i>
                </span>
                    <span class="is-visually-hidden-touch is-hidden-portrait">Feedback</span>
            </button>
        </li>
        <li id="share-menu-link">
            <div class = "sharing dropdown has-caret">
                <button class="dropdown-trigger button is-text is-fullwidth has-flex-justify-content-start has-inner-focus is-small is-icon-only-touch colorfill" title="Share This Document">
                    <span class="icon">
                        <span class="iconify extra-small" data-icon="ls:share" data-inline="false"></span>
                    </span>
                    <span class="is-visually-hidden-touch is-hidden-portrait">Share</span>
                </button>
                <div id="sharing-menu" class="sharing-menu has-padding-small is-hidden">
                    <ul>
                        <li id="twitter-link">
                            <a class="button is-text is-fullwidth has-flex-justify-content-start has-inner-focus is-small share-twitter" href="" target="_blank">
                                <span class="icon effect">
                                    <i class="fab fa-twitter colorfill"></i>
                                </span>
                                <span>Twitter</span>
                            </a>
                        </li>
                        <li id="linkedin-link">
                            <a class="button is-text is-fullwidth has-flex-justify-content-start has-inner-focus is-small share-linkedin" href="" target="_blank">
                                <span class="icon effect">
                                    <i class="fab fa-linkedin-in colorfill"></i>
                                </span>
                                <span>LinkedIn</span>
                            </a>
                        </li>
                        <li id="facebook-link">
                            <a class="button is-text is-fullwidth has-flex-justify-content-start has-inner-focus is-small share-facebook" href="" target="_blank">
                                <span class="icon effect">
                                    <i class="fab fa-facebook-f colorfill"></i>
                                </span>
                                <span>Facebook</span>
                            </a>
                        </li>
                        <li id="email-link">
                            <a class="button is-text is-fullwidth has-flex-justify-content-start has-inner-focus is-small share-email" href="" target="_blank">
                                <span class="icon effect">
                                    <i class="fas fa-envelope colorfill"></i>
                                </span>
                                <span>Email</span>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
        </li>
    </ul>
</div>

<!-- end of / Page Functions -->
    </div>
    <div class="progress-container">
        <div class="progress-bar-scroll" id="myBar"></div>
    </div>
</div>

<!-- end of /.header-holder -->
			<!-- Main Container --> 
<div class="mainContainer uhf-container  has-top-padding ">
    <div class="columns has-large-gaps is-gapless-mobile ">

        
        <!-- Left Hand Side Tree-->
        <div class="left-sidebar column is-hidden-mobile 
         
        is-one-fifth-tablet is-one-seventh-desktop 
        ">
            <nav class ="is-fixed is-flex is-flex-column" id="affixed-left-sidebar">

                <!-- Include LHS Menu -->
                
                <a href="../article-search.html">
                    <span id="earlierPage" class="is-primary">
                        <span class="iconify medium" data-icon="ic-sharp-arrow-back" data-inline="false"></span>
                    </span>
                </a>
                
            </nav>
        </div>
         <!-- end of /left hand side tree -->
        

        <!-- Primary Holder -->    
        <div class="primary-holder column 
        
         is-four-fifths-tablet is-six-sevenths-desktop 
        ">
            <div class="columns has-large-gaps is-gapless-mobile 
            
            ">

                 <!-- Main Column -->
                <div id="main-column" class="column 
                 
                is-full is-four-fifths-desktop 
                ">
                    <main id="main" class ="
                    
                        content docs articles-and-blogs
                    " lang="eng-us">
                        <h1 id="soul-machines">Soul Machines</h1>
<time class="metadata" style="text-alstyleign:left"> Articles &amp; Blogs • Main Category • 06/05/2021</time>

<figure>
    
        <img class="docimage" src="images/soul-machines.jpg" alt="" style="width: 100%; max-width: 85.7%" />
    

    
    <figcaption style="text-align: center; color: #5e5e5e; font-size: 0.9rem;">
        <i></i>
    </figcaption>
    
</figure>

<p>This week I spent some time learning more about the V.F.X. Industry (an industry I’ve always been very passionate about!). I saw on my Weta Digital Feed a post inviting all of its followers to attend the ‘Real Time Virtual Conference’, where they could learn more about some of the technologies being used by V.F.X. houses to create Digital Humans, and challenges too being faced by V.F.X. houres, when creating real-time renders. The usual conference fee of $300 had been waivered, so it was something I could afford to go to.</p>

<p>Many innovators and leaders in C.G.I., from across the globe, were there to discuss (through a series of live panel discussions, and Q&amp;A sessions) the growth potential of real-time technologies. When I logged in, I was directed to a page that had 4-hours of panel discussions and interviews on various emerging technologies in development. Talks included: how to create Real-Time Humans with 4-D Faces &amp; Body Sims (<strong><em>Ziva Dynamics</em></strong>); the Future of Volumetric Capture (<strong><em>MetaStage</em></strong>); some of the challenges currently being faced to create Autonomous Digital Humans (<strong><em>Digital Domains</em></strong>); how digital humans were created on C.G.I. live action films such as Alita Battel Angel, and Gemini Man (<strong><em>Weta Digital</em></strong>); to the talk I was most impressed by: ‘Adding a Touch of Soul’ to the ‘Metaverse’ (virtual spaces such as those portrayed in the films Ready Player One and The Matrix), by Mr. Mark Sagar of Soul Machines. His company creates Animated Virtual Humans, but not Animated in the traditional sense (by Human Beings), but Animated by Artificial General Intelligence, such that they can interact and respond to their environment and those around them COMPLETELY on their own – an idea, I believe, has the potential to revolutionise the way many industries work.</p>

<p>When you consider the use-cases, you can’t help but feel astounded by the possibilities – for example, in the Technology &amp; Media industry, the ability to create Digital People (customer service representatives or assistants) that can answer questions on any number of topics, using A.G.I. as a front end for all interactions. In the Entertainment industry, the ability to create digital avatars of whoever you like – Singers, Musicians, Actors &amp; Actresses, Sportsmen &amp; Sportswomen, Celebrity Scientists &amp; Engineers, or even if you wanted to, your own relatives. Then in the Healthcare Industry, creating Digital Human healthcare assistants that can speak only (consensus) facts in a compassionate, sensitive and empathetic manner. Then the Education industry, where Digital People can help organisations improve their own learning platforms, with similar applications to adjacent Consumer Goods, and Real Estate industries.</p>

<p>In fact, I was so impressed by what the talk, and what the company is trying to achieve, its mission, its vision, that I am writing down here a transcript of the full presentation and interview that Mr. Mark Sagar, the founder of Soul Machines, gave to the audience and panel participants (Christophe Hery – Research Scientist, Facebook Reality Labs, and Mike Seymour – Co-Founder of the FX Guide and Lecturer, Researcher of Digital Humans).</p>

<p>It’s a truly fascinating, eye-opening, and thought-provoking talk. If you want to learn more about Soul Machines, and the amazing work that Mark Sagar and his team are doing in the field of Digital Humans, and Artificial General Intelligence, (and some of the commercial, or corporate applications), you can visit their website below, to find out more:</p>

<p><a href="https://www.soulmachines.com/" target="_blank" class="no_icon">Soul Machines</a></p>

<p>Here is the transcript of the interview:</p>

<p><strong>Introduction – by Christophe Hery:</strong></p>

<p>“We have the privilege to get Mark Sagar, an ending keynote for this day. I’ve known Mark since the late 1990s. He filmed the Jester, and pioneered the expression and rendering of Digital Humans.</p>

<p>In 2006, I was fortunate enough to invite him to a talk at F.M.X., along with Paul Ekman. This is probably one of the last trips that Paul Ekman did overseas. And it was amazing! We all witnessed there a great exchange between brilliant minds, and what I would call the father, and potentially the son of the facial action coding system. Mark was arguing that there may be a missing activation unit in Paul’s seminal work.</p>

<p>Several years later, still at F.M.X. in 2015, Mark came to present his BabyX, and the audience was stunned again! We had somebody who was talking about chemicals in the brain, and how the dopamine can change the behaviour, simulating all of that, and doing that with a representation of his daughter. So, I am very glad to hear him again tonight, explaining his journey, with Digital Humans and Digital Brains, and likely opening New Research Areas for all of us in this field.“</p>

<p><strong>Talk by – Mark Sagar</strong></p>

<p>“Thank you very much for that very kind introduction Christopher, it seems the distant past that we were flying to conferences and enjoying each other’s company – in real-life 3-D.</p>

<p>I guess, in terms of the Pandemic, with the increased use of Digitisation, we’ve kind of reached a tipping point that has pushed us that much closer to adopting these sorts of technologies, that it has become a normal way of interacting now.</p>

<p>The Metaverse’s time has really come, and all of the enabling technologies have reached the appropriate point.</p>

<p>I will just start my presentation. Hopefully, I am going to share the right screen because it is always embarrassing to get the wrong one. Please, somebody, tell me if it doesn’t say what I think it does.</p>

<p>What I’d like to talk about now is how do we populate the Metaverse. In the original ‘SnowCrash’ book, users were appearing as avatars (a graphical representation of a user) in the Metaverse, but I’d imagine that the vast majority of the Metaverse is going to be populated by agents, human-like artificial intelligence agents, that are representing people, or organisations, so you can interact with people at a much greater scale than before.</p>

<p>Why do you want to make them Human Like?</p>

<p>Human Like beings are the most natural thing for us to interact with. It is the most efficient form of communication and cooperation.</p>

<p>When I was thinking about this talk I remembered a short film that we did twenty years ago at a company, called Life Effects. It was motivated by a debate between Bill Joy and Ray Kirstwhile on the consequences of the technology singularity (a hypothetical point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable changes to human civilization) – (not be confused with gravitational singularity, where the density and gravitational field of a celestial body is thought to be or become infinite)</p>

<p>Now, this movie is appropriate to the meta verse on multiple levels, because it is basically about artificially intelligent agents interacting in a shared space, and it makes use of ‘real-time technology’. This was 20 years ago. We made the film by recording real-time agents running on a screen. They had automated lip sync, and they actually used a script that was partially written, by algorithms. The characters are anthropomorphised (attribution of human traits to non-human entities) versions of A.I. programs that we may one day come across in the Metaverse.</p>

<p>So here we go..</p>

<p>CLIP SHOWN HERE</p>

<p>Alright, well thanks for putting up with the bad jokes. This (the contents of the CLIP) touches on a serious issue.</p>

<p>To me I think it is a no-brainer that we need to be responsible about the design of artificial intelligence. I believe it should be used for human empowerment, and not to disadvantage people. We of course still need to keep humans in the loop as much as possible, so that there is a chain of responsibility.</p>

<p>I am a positive futurist. I think that A.I., if used properly, can bring enormous benefits to humanity. Now, human cooperation is the most powerful force in history. Human Brains, working together, have enabled us to put people on the moon. We’ve created micro-chips, vaccines, all kinds of incredible technologies, which have advanced us and made us the most powerful force on the planet.</p>

<p>‘<em>Human Co-operation with intelligent machines will define the next era of history</em>’.</p>

<p>So, the business of “how do we co-operate with machines?”, is really the mission of ‘Soul Machines’, the company - where we are developing digital people that can provide an intelligent, and empathetic interface between human and machine, so it can be used for all kinds of purposes.</p>

<p>I’ll show a quick intro video:</p>

<p>CLIP STARTS</p>

<p>Has slogans like:</p>

<p>•	You’ve always imagined
•	Computers being more human
•	Now they are!
•	We are building ‘Digital Heroes’ that can learn, and react in ‘Real Time’
•	Driven by a Digital Brain – that can autonomously animate them
•	Our Digital Heroes have ‘Personality, and Character’ – when you talk face to face
•	Make your next team mate a ‘Digital Hero’ (e.g. Automotive Assistant, Teacher, Influencer, Police Officer, Banking, Customer Service, Leasing, Doctor)
•	Create the time to do all things you don’t have time for, or don’t want to
•	Our Digital Heroes are here to help!</p>

<p>CLIP ENDS</p>

<p>I’d just highlight the diverse range of projects that we are currently working on. Here is an example we’ve done for the World Health Organisation, which is basically an assistant for Covid, who dispels myths, and also helps you quit smoking, because it turns out the Covid plus smoking is a disastrous combination.</p>

<p>CLIP SHOWN</p>

<p>So, Florence (the Digital Human shown in the clip) is working around the world in multiple languages, at the moment.</p>

<p>The other thing you can do of course, is that you can make digital twins of people. You can make virtual assistants that have a little bit more personality than Siri or Alexa.</p>

<p>CLIP SHOWN (with the Singer, Will I Am)</p>

<p>The cool thing is you can also create these Digital People that can work in any kind of different language. So, here, for example is one that works in Japanese, but also runs in America, and speaks English.</p>

<p>CLIP SHOWN (of person speaking in Japanese)</p>

<p>So, these are the things currently in operation, but where do we go next?</p>

<p>To really make the Metaverse, to unleash the full power of it, ‘we want to have truly autonomous Virtual Characters’, where these characters can also have experiences, and actually talk to you about those experiences. They are characters you can co-operate with, you can do creative things with, you can do all kinds of stuff, just like you can do with other people.</p>

<p>And to bring together all the elements of Human Co-operation, you basically have to give the characters Digital Brains. The Digital Brains and Artificial Nervous Systems co-ordinate all of the behaviour and make it coherent. 
(It also needs to overcome the ‘Uncanny Valley’).</p>

<p>And just like we’ve got an ‘Uncanny Valley’ for facial appearance (the region of ‘human likeness’ where humans have a negative emotional response towards robots that appear “almost” human), we’ve also got it for movement, we’ve got it for the voice, we’ve got it for the appropriateness of behaviour. It just keeps going….</p>

<p>…So, we’re just at the very tip of the Uncanny Valley. The Uncanny Valley, is actually just an Uncanny Mountain Range, that we have to cross on many, many different levels.</p>

<p>And we believe that the way to do this properly is to try and emulate the Human Brain. So, we’ve been building brain models, and using them to drive digital characters.</p>

<p>I am now going to show some clips from a live TV show.</p>

<p>It is the first time where a presenter has interacted live with an Autonomous Virtual Human, driven by a Virtual Brain.</p>

<p>CLIP STARTS ABOUT BABY X</p>

<p>Transcript of clip</p>

<p><strong>With Nigel Latta, Psychologist / Presenter, and Mark Sagar.</strong></p>

<p>“This is Baby X. She is one of the most advanced Brain Simulations in the world, and she is enjoying playing a Peeca Boo game, with me. So, what is Baby X?</p>

<p>“It is a virtual infant simulation. It is trying to create the elements which put together the aspects of what makes ‘something’ lifelike” – Mark Sagar</p>

<p>BabyX isn’t an animation. She is a Virtual Human, and all of the behaviours are generated by Virtual Humans. She watches and learns, and listens to what I do, and makes her own decisions in real-time about how to respond.</p>

<p>If I teach her that this is a Duck using associative learning. Then something amazing happens that we do as well, but take completely for granted.</p>

<p>Now watch what happens to BabyX’s brain when we say the word duck to her. Her brain builds a link between the word ‘duck’ and an actual ‘duck’.</p>

<p>We can see that BabyX is looking at the Spider and there is no reaction. But I am now going to tell her it is something she should be scared of. ‘Scary Spider!’. The Spider now provokes a clear fear response. The parts of the spider triggers specific parts of the brain. Her Amigdala initiates a cascade of reactions which sends chemicals into her system, that generates physical and emotional feelings of fear. When we take the spider away, she calms down. And when we bring it back, we can see she is now scared of it.</p>

<p>When BabyX presses the green button, a Duck appears. When she presses the red button, a Snake appears. As BabyX continues to press the buttons, her brain continues to form new connections. She wants to see a Duck. So, she is working out that pressing the green button gets her what she wants.</p>

<p>This ability to learn sequences and predict what might be about to happen next, is crucial to everything we do!</p>

<p>We know that making eye contact is a powerful way to create a sense of connection. There is a chemical in your brain called Oxytocin (the Hug Drug) that also has an amazing effect on how we connect with other people.</p>

<p>I am going to increase her levels of Oxytocin, and let’s see what affect that has. BabyX makes a clear shift to focus on eye contact. We’re connecting, and she even gives me a smile. Inside BabyX’s brain, we can see Oxytocin being released. Here it is the greeny blue bits secreted by her pituitary gland.</p>

<p>This simple act of connection means she is rewarded with Oxytocin, which make her seek even more connection. And even though I am interacting with a Digital Baby, the same thing is happening inside my brain.<br />
“</p>

<p>CLIP ENDS…</p>

<p>…with a slogan – we are building machines like us to engage with us, we are building machines like us to collaborate with us, we are building machines like us to create us.</p>

<p>So that was quite an entertaining experience, recording a live TV show with an Autonomous Digital Baby that doesn’t necessarily do that you want it to do. But it was fascinating because it was all live, and a fundamentally different experience because one of the people there wasn’t a real person.</p>

<p>We are continuing to work on BabyX. We are basically doing the equivalent of an early behaviour Turing test, where we have been recording real Parents or Care-Givers interacting with their children, and recording real Babies interacting in shared digital media. And we have been setting it up so we can replace the real Baby with Baby X. And if we are able to elicit the same behaviours out of the Care-Givers and Parents, with Baby X, then effectively we have closed that loop, and validated the behaviour affects. So we are really looking at the basis of co-operation between Human and Machine.</p>

<p>Anyway, that is the end of my talk, so thank you very much.<br />
“</p>

<p>“Thank you Mark, this is great as you know. Very mind boggling. I know Mike is appearing, I am sure he wants to ask something”</p>

<p>“I’d ask Mark about 1,000 questions on any day of the week. I guess Mark, you chose very deliberately to pick a baby to do this initial work. And of course, its brilliant! But can you run the clock forward on the more complex interactions, if you increase the domain of expertise that you want the Digital Character to react to”</p>

<p>“Yes, we do a range of different things. For example, when we are doing things like a Virtual Therapist working on a Cognitive Behavioural Therapist, those types of technologies are much more curative experiences, where it is about guiding people through things, and you don’t want them running completely autonomously and doing random things.”</p>

<p>“However, if we think about the future of games, we truly want to have characters and games, where there is a consequence to how we interact with them. Where they have memories, where it (how we react with them) changes what happens next. That’s how it is with real people.</p>

<p>With real people, everything matters so much, because there is a consequence to every moment. That moment becomes part of the shared history. And digital characters need to have that same set of memory. You’re building things which become important, because they affect things. And that’s the essence of how do you build those memories? How do you build all the sensors? How do you build those models that make sense of the world?</p>

<p>With BabyX, we are looking into the nature of how humans classify events? How we process them? How do we convert those to language? So, there is so much that is not known yet, that the Baby model is the correct, I believe, the right level, to start exploring this space, because there are so many levels of things.</p>

<p>If we think about language, at the moment. If you look at typical N.L.G. (Natural Language Generation) type of models, trained on tonnes and tonnes of data, with different types of bots (Blender Bot, or GPT-3, and so on) - those types of models are vast big statistical models, and what you’re really looking for is, given what’s just happened, is what is proper to follow.</p>

<p>It is not what you call “grounded meaning”. The language response is not grounded in the experience of the character (but something else?). When we talk to other people and relate to other people, a lot of that is about: I want to know more about your experience, sharing human to human; this is what I saw, this is what it felt like.</p>

<p>With BabyX what we’re trying to do is create a Machine that experiences, and which learns on the fly, so that every (person) interaction is completely different, it is co-created.”</p>

<p>“I totally agree with you about the importance of co-creation. But if I could just spend the focus of our attention to the commercial side to some of the amazing work you have been doing and work that Soul Machines is doing.</p>

<p>I’m wondering, and I’m not talking about any of your clients in particular, I’m talking about a client in the abstract. But, if I came to you with a client with a problem, and I wanted you to produce a Digital Human, is there not a temptation that I am going to want that representative of my company to be perky and happy, and just almost always smarmy, all the time, because my first reaction is ‘Yes, my Digital Employee should be Happy all the time’.</p>

<p>Yet, even in a real world situation, with the best front of house employee, they don’t just smile at you like an idiot, like a Cheshire cat the whole time. The realism from the matching that you know about at BabyX, does it have a place in the Corporate application?”</p>

<p>“Yes, I think what you’re talking about is Empathy, and if you think about a typical Customer Service Representative, for example, you want your emotions to be acknowledged. You want to be heard. If I went up to a Hotel Counter and started complaining about stuff, I wouldn’t want the Hotel Concierge or whatever, to smile at me at that point. It is the exact wrong thing. I want my emotions to be acknowledged.</p>

<p>You’re taking into account the acknowledgement of the emotion, and you’re responding to it, and I think that is the key thing. As humans, we are operating on multiple levels. We are emotional beings, we are rational beings, we’ve got goals, we’ve got these different things. You’re trying to acknowledge all the different aspects of what that person is.</p>

<p>The Digital Human cannot possibly understand your life. They can’t understand human suffering because they’re not human, they haven’t had a human life. However, they can respond in a way that takes that into account. You would hope that the organisation behind that, that matters to them. That’s the value in it. The customer is irate. They are upset. Their emotion matters. It counts for something. And it may change the recommendation, or whatever the Digital Human does, it might change how it responds, but that is actually a considered thing. It is a kind of holistic customer care if you like.”</p>

<p>“Because I think one of the things you have done so effectively is the effective Computing Lit. Like Baby X reads a pad in front of you. Your face, your tone, your whatever, and obviously there is matching, and there are responses there. But in a Customer Service environment, I could be angry and raise my voice and yell. But I could also be angry and be very quiet, and very not moving.</p>

<p>It seems to me that a lot of people assume that computers can easily read real emotions off humans. What they tend to read very well are pantomime emotions (theatrical emotions) off humans. Would you like to comment on that?”</p>

<p>“One of the things is that, you’ve probably seen a film clip, and you change the music, and it changes the entire emotional meaning of something. So, if I sat here like this, you know, it might actually depend on. Actually, it completely depends on context. What was I doing just before that? It might have been, like that or something like that. Or whatever. So, what’s just happened, affects the context of what’s just happening now. It might actually just be an improvement.</p>

<p>The other thing is that it is about coherence. So, there are people, and Paul Ekman has written lots about this. Trying to tell if someone is telling a lie or not, you are looking at their facial behaviour. You are looking at their tone of voice. And you are also looking at the semantics of what they are saying. And you are looking for coherence between all the different channels of information. Is it appropriate to what you are saying?</p>

<p>Police, when they are interrogate people, they might ask lots and lots of questions, where they are getting an answer, they’re getting a pattern of an answer, and then they suddenly throw in something, that the suspect is not suspecting, and then they really carefully watch what they do. Paul Ekman use to say, it increases the cognitive load on the person, and that tends to show in their face or their voice. That’s in a real person. So, for all of these things, you need to take into account all of the information.”</p>

<p>“Because, I mean the Counter-Point to that would be Malcolm Gladwell’s assertion that even C.I.A. interrogators are hopeless at whether people are lying or not, and that it’s incredibly hard for us to decode Human Expressions, and we’re meant to be really good at it.”</p>

<p>”Exactly, and I think Machines will have exactly the same problem. One of the things that we’re doing, all the time, is that we’re doing Theory of Mind. We’re trying to simulate what this other person might be thinking about. We’re trying to work out what that person’s intention is. Right. And so, that actually requires us to have a brain model of another person in our own brain. And this is one of the things we are exploring with BabyX. This is one of the next steps. Intentionality. So, all of these things actually have so many layers, and that’s why it is so difficult for us to tell what another person is really thinking.</p>

<p>And then, when it comes to other people, it comes down to on the foundations of trust, consistency. We have experience of other people, and then we get a formation of what we might think their character is. And character, I like to think of as a stable weather system. You might have the weather in Australia for example, which has a particular pattern, maybe not in the last year or two, but basically, that is a characteristic of Australia. Now, if you went to Australia and suddenly it started snowing in the middle of the desert, then you’d go: ‘something is really up here’. This doesn’t make sense! And it’s the same thing with a person’s behaviour.</p>

<p>When I was talking about the uncanny valley stuff before it’s going to come down to, is that person acting in character. This is where all of this stuff is going to need to go. Then when we’re creating digital characters, they are their own entities. I think that is going to become some of the most interesting stuff. I wonder if in the future we’ll be able to talk to a machine and ask it, what was it like to experience the internet all at once, or something like that.”</p>

<p>“Have you done some projections, Mark, about the amount of the memory you might need. Like say, I’m talking today with BabyX. And somehow you have a way for her to record the meaning of the conversation, or some of the topics we’ve been talking about. And then a year from now, I want to pick up that conversation. I remember that we talked about it. Have you done some back of the envelope calculation about the amount of memory. This is kind of an abstraction again of the brain, how its made..”</p>

<p>”So, one of the things that we’ve done a fair bit of research on, is event representation in the brain. Because brains are very, very efficient at storing information. So, we don’t remember absolutely everything. For example, if you walked, and you think about how you came into the office this morning, you’re only going to remember something if it stood out. So, you’re only going to remember eventful things. So that’s the brains way of compressing information for example.</p>

<p>Everything that we do. The encoding and the abstraction of things is a way of making information – about reducing the dimensionality of it. What our brain is all about, it’s all about, a dimensionality reduction machine.</p>

<p>We have streams and streams of data coming in. I’d say, through a camera. All the pixels coming in, and, can we say convert that into an object representation. If I can convert that into an object representation, I’ve actually got a label on it. I’ve got a concept about that object. So, what Baby X’s memory does, it is reconstructive. So, if that object concept gets triggered, it basically reconstructs what that object would be. It doesn’t actually have to store all those things, it is really the parameters to recreate it.</p>

<p>Once you start getting down to representing an object, you’re converting sub-symbolic (connectionist, neural network approach) to symbolic (object like, hierarchical logical approach). At that point things start becoming a lot more efficient. We do this all the time with the amount of information we get. It actually blows my mind, to think about it. You know, you just think about your visual system. It’s going constantly, those neurons are constantly firing. Doing all kinds of computation, really, and recognising all things, and it is powered by little electrical signals and chemicals moving around. It is miraculous.</p>

<p>So, the type of computation for this, ultimately, I think, will be the promise of things like <em>neuromorphic computing</em> (concerned with emulating the neural structure and operation of the human brain), which is when things are triggered based on actual events happening. Everything is sitting quiet, until it gets triggered. Because typical computers are processing all the time. They are burning off tonnes of energy. That’s far into the future. People are working on these sorts of technologies now, but it’s a ways into the future.</p>

<p>But, in terms of the other things you were talking about, the amount of memory, I guess, the human brain has gazillions of connections, and we (at BabyX)  don’t have anything with that type of connectivity at the moment because we haven’t built that out of silicon. So, there is still a long way to go. Memory truly blows me away. Our ability to access it and recreate all these things. Part of it is as part of the architecture of how our brain is accessing information, and re-constructing, and sorting, and planning all these things. It’s all super juicy stuff!”</p>

<p>“Fascinating, for sure!”</p>

<p>“Well Mark, I think that your contribution is spectacularly good! As always, we thank you so much for doing it. I suddenly realised that we’re almost out of time, but if we could just get you back for another 17 hour session, we’d enjoy it”</p>

<p>“Did you get my Australian joke.”</p>

<p>“I didn’t actually because. No, I didn’t hear that at all actually. No check mate here mate.”
“Was that about the weather?”</p>

<p>“New Zealanders taking swipes of an Australian is awfully funny, but then I’m always amused when my dog tries to do new tricks. …Oh, did I say that out aloud”</p>

                        
                        
                    </main>	
                    
                        <!-- Start of Feedback section -->
<section class="feedback-section is-relative">
    <h2 id="feedback" class="is-size-2 has-margin-top-large has-padding-top-medium"> Feedback </h2>
    <div class="feedback has-margin-top-small has-margin-bottom-none">
        <p id="send-feedback-about" class="has-margin-top-none has-margin-bottom-none">
            Submit and view feedback
        </p>
        <div id="feedback-section-link" class="has-margin-top-large has-margin-bottom-none">
            <a class="button has-margin-bottom-small" href="" target="_blank">
                <span class="icon">
                    <span class=github>
                        <i class="fab fa-github"></i>
                    </span>
                </span>
                <span>This page</span>
            </a>
        </div>
    </div>
    <div id="viewallfeedback-section-link" class="is-flex has-flex-justify-content-end has-margin-top-small has-margin-bottom-medium">
        <a class="view-on-github" href="" target="_blank">
            <span class="icon">
                <span class=github>
                    <i class="fab fa-github"></i>
                </span>
            </span><span>View all page feedback</span>
        </a>
    </div>
</section>

<!-- End of Feedback section -->
                    
                </div>

                
                 <!-- Right Hand Side Toc-->
                 <div class="right-sidebar column is-one-quarter is-one-fifth-desktop is-hidden-mobile is-hidden-tablet-only">
                    <nav class="doc-outline is-fixed is-vertically-scrollable", id="affixed-right-sidebar">
                        <nav id="side-doc-outline">
                            <header id="side-doc-title" class = "is-flex is-flex-row">
                                <div>
                                    <a href="#top">
                                        <h3 title="Go to Top of Page">
                                            <i class="fas fa-file-alt" style="color:#e82127;">&nbsp;</i>
                                            In this article
                                        </h3>
                                    </a>
                                </div>
                            </header>
                            <ul class="section-nav">
</ul>
                        </nav>

                        <!-- Fixed Element-->
                        <div>
                            <a class="top-link hide" href="" id="js-top" title="Go to Top of Page">
                                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6"><path d="M12 6H0l6-6z"/></svg>
                            </a>
                        </div>
                    </nav>
                </div>
                
            </div>
            <!-- end of /.columns -->
        </div>	
        <!-- end of /primary holder -->

    </div>
    <!-- end of /.columns -->
</div>
<!-- end of /.mainContainer -->
			<!-- Footer Container --> 
<div class="footerContainer">

    
    

    <footer id="footer" class=" footer-layout   uhf-container has-padding">
        <div class="">
            <a href="https://startech-enterprises.github.io/docs/">
                <img src="/../docs/assets/images/starbase_logo.png" class="footer-bar-logo">
            </a>
        </div>
        <div class="is-flex is-full-height has-padding-right-extra-large-desktop has-padding-left-extra-large-desktop">
            <a class="locale-selector-link has-flex-shrink-none" href="/../">
                <span class="icon">
                    <span><i class="fas fa-globe"></i></span>
                </span>
                <span class="locale-selector-link-text">English (United Kingdom)</span>
            </a>
        </div>
        <ul class="links">
            <li>
                <a href="/docs/">Docs</a>
            </li>
            <li>
                <a href="/docs/">Guides</a>
            </li>
            <li>
                <a href="/docs/articles-and-blogs/article-search.html">Articles & Blogs</a>
            </li>
            <li>
                <a href="/docs/videos/">Videos</a>
            </li>
            <li>
                <a href="/docs/about/about.html">About</a>
            </li>
            <li>
                <a href="https://www.linkedin.com/company/startech-enterprises/" target="_blank">LinkedIn</a>
            </li>
            <li style="color: #e82127;">
                <span class="icon">
                    <span> <i class="fas fa-star"></i></span>
                </span>Star Base 2020
            </li>
        </ul>
    </footer>
</div>
<!-- end of / footerContainer -->
	</body>
		<!-- Reference to main JavaScript Files -->
<script src="/../docs/assets/js/library.js" type="application/javascript"></script>
<script src="/../docs/assets/js/breadCrumb.js" type="application/javascript"></script>
<script src="/../docs/assets/js/navbar.js" type="application/javascript"></script>
<script src="/../docs/assets/js/affix.js" type="application/javascript"></script>
<script src="/../docs/assets/js/rhsToc.js" type="application/javascript"></script>




<script src="/../docs/assets/js/scrollTop.js" type="application/javascript"></script>
<script src="/../docs/assets/js/gitFeedback.js" type="application/javascript"></script>
<script src="/../docs/assets/js/copyCode.js" type="application/javascript"></script>
</html>
